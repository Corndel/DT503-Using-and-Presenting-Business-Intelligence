{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E-commerce Customer Analytics and Personalization Project\n",
        "\n",
        "## Overview\n",
        "This notebook covers advanced analytics techniques for e-commerce data, including **customer segmentation**, **cohort analysis**, **churn prediction**, and **personalized marketing recommendations**. These techniques help businesses understand their customers better and make data-driven decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Preparation and Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 Importing Libraries\n",
        "\n",
        "We'll start by importing the necessary libraries for our analysis. These libraries provide tools for data manipulation, visualization, and machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import basic data manipulation and visualization libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import machine learning libraries\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Import other necessary libraries\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Set the environment variable to avoid memory leak issue on Windows with MKL\n",
        "os.environ['OMP_NUM_THREADS'] = '1'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running this code block, we have imported all the necessary tools for our analysis. Here's a breakdown of what each library does:\n",
        "\n",
        "- **pandas** (as pd): A powerful data manipulation library. We'll use it to load, clean, and analyze our dataset.\n",
        "- **numpy** (as np): A library for numerical computing. It's used for array operations and mathematical functions.\n",
        "- **matplotlib.pyplot** (as plt) and **seaborn** (as sns): These are visualization libraries that we'll use to create graphs and charts.\n",
        "- **sklearn**: This is the Scikit-learn library, which provides tools for machine learning. We've imported specific modules for clustering (KMeans), data preprocessing (StandardScaler), model training (train_test_split), classification (RandomForestClassifier), and model evaluation (classification_report).\n",
        "\n",
        "The last two lines set up our plotting style to make our visualizations consistent and visually appealing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Loading and Cleaning Data\n",
        "\n",
        "Now we'll load our e-commerce data and perform some initial cleaning steps. Data cleaning is a crucial step in any data analysis project, as it ensures our data is accurate and ready for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "df = pd.read_csv('../ecommerce_data_modified.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(\"First few rows of our data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running this code, we've accomplished two important tasks:\n",
        "\n",
        "1. **Data Loading**: We've used pandas' `read_csv()` function to load our e-commerce data from a CSV file into a DataFrame called `df`. This DataFrame is now our main data structure for analysis.\n",
        "\n",
        "2. **Initial Data Inspection**: \n",
        "   - We've printed the first few rows of the data using `df.head()`. This gives us a quick look at the structure of our data and the types of information we're working with.\n",
        "   - We've checked for missing values using `df.isnull().sum()`. This shows us how many null or missing values are in each column, which is crucial information for our data cleaning process.\n",
        "\n",
        "Understanding the structure of our data and identifying missing values are critical first steps in data analysis. They help us plan our cleaning and preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert order_date to datetime\n",
        "df['order_date'] = pd.to_datetime(df['order_date'])\n",
        "\n",
        "# Fill missing values in categorical columns with 'Unknown'\n",
        "categorical_columns = ['customer_age_group', 'customer_region', 'web_traffic_source']\n",
        "df[categorical_columns] = df[categorical_columns].fillna('Unknown')\n",
        "\n",
        "# Fill missing numerical values with median\n",
        "numerical_columns = ['customer_lifetime_value']\n",
        "df[numerical_columns] = df[numerical_columns].fillna(df[numerical_columns].median())\n",
        "\n",
        "print(\"Data types after cleaning:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this code block, we've performed several important data cleaning steps:\n",
        "\n",
        "1. **Date Conversion**: We converted the 'order_date' column to datetime format. This allows us to perform time-based operations and analysis more easily.\n",
        "\n",
        "2. **Handling Missing Values**: \n",
        "   - For categorical columns (like 'customer_age_group', 'customer_region', and 'web_traffic_source'), we filled missing values with 'Unknown'. This is a common practice when we don't want to lose data but can't make assumptions about the missing values.\n",
        "   - For numerical columns (like 'customer_lifetime_value'), we filled missing values with the median of that column. We use the median instead of the mean because it's less sensitive to outliers.\n",
        "\n",
        "3. **Data Type Check**: After our cleaning steps, we printed the data types of our columns to ensure that our changes were applied correctly and that each column has the appropriate data type.\n",
        "\n",
        "These cleaning steps prepare our data for analysis by ensuring that there are no missing values and that our data types are appropriate for the kind of analysis we want to perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Exploratory Data Analysis\n",
        "\n",
        "Now that our data is clean, let's perform some basic **exploratory data analysis** (EDA) to understand our dataset better. EDA helps us identify patterns, spot anomalies, test hypotheses, and check assumptions through summary statistics and graphical representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Basic statistical summary of numerical columns:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code provides a statistical summary of our numerical columns:\n",
        "\n",
        "- **count**: The number of non-null values in each column.\n",
        "- **mean**: The average value.\n",
        "- **std**: The standard deviation, which measures the amount of variation in the dataset.\n",
        "- **min** and **max**: The minimum and maximum values in each column.\n",
        "- **25%, 50%, 75%**: The quartiles of the data. 50% is the median.\n",
        "\n",
        "This summary gives us a quick overview of the distribution of our numerical data. We can see the central tendencies, spread, and potential outliers in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize distribution of revenue\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(df['revenue'], kde=True)\n",
        "plt.title('Distribution of Revenue')\n",
        "plt.xlabel('Revenue')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This visualization shows the distribution of revenue in our dataset:\n",
        "\n",
        "- The x-axis represents different revenue values.\n",
        "- The y-axis shows the frequency of each revenue value.\n",
        "- The blue bars represent a histogram of the data.\n",
        "- The overlaid line is a Kernel Density Estimate (KDE), which provides a smoothed version of the histogram.\n",
        "\n",
        "From this plot, we can observe:\n",
        "1. The shape of the distribution (Is it normal? Skewed?)\n",
        "2. Any potential outliers or unusual patterns\n",
        "3. The range and central tendency of our revenue data\n",
        "\n",
        "This kind of visualization is crucial for understanding the overall behavior of our key metrics like revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize relationships between numerical variables\n",
        "numerical_cols = ['revenue', 'product_price', 'quantity', 'discount', 'customer_lifetime_value']\n",
        "sns.pairplot(df[numerical_cols], height=2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pairplot visualizes the relationships between our key numerical variables:\n",
        "\n",
        "- Each scatter plot shows the relationship between two variables.\n",
        "- The diagonal shows the distribution of each variable.\n",
        "\n",
        "From this visualization, we can:\n",
        "1. Identify correlations between variables (positive, negative, or no correlation)\n",
        "2. Spot any unusual patterns or clusters in the data\n",
        "3. Get a sense of the distribution of each variable\n",
        "\n",
        "This is particularly useful for understanding how different aspects of our e-commerce data relate to each other. For example, we might see how product price relates to quantity sold, or how customer lifetime value relates to revenue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize average revenue by product category\n",
        "plt.figure(figsize=(12, 6))\n",
        "avg_revenue_by_category = df.groupby('product_category')['revenue'].mean().sort_values(ascending=False)\n",
        "avg_revenue_by_category.plot(kind='bar')\n",
        "plt.title('Average Revenue by Product Category')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Average Revenue')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This bar chart visualizes the average revenue for each product category:\n",
        "\n",
        "- The x-axis shows different product categories.\n",
        "- The y-axis represents the average revenue for each category.\n",
        "- The bars are sorted in descending order of average revenue.\n",
        "\n",
        "From this visualization, we can:\n",
        "1. Identify which product categories generate the highest average revenue\n",
        "2. Compare the performance of different product categories\n",
        "3. Spot any categories that significantly outperform or underperform compared to others\n",
        "\n",
        "This information is valuable for strategic decision-making, such as where to focus marketing efforts or which product lines to expand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Customer Segmentation\n",
        "\n",
        "In this section, we'll perform **customer segmentation** using the K-means clustering algorithm. Customer segmentation is the practice of dividing a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, interests, and spending habits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Feature Selection and Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select relevant features for segmentation\n",
        "features = ['customer_lifetime_value', 'revenue', 'quantity']\n",
        "\n",
        "# Group by customer_id and calculate mean of selected features\n",
        "X = df.groupby('customer_id')[features].mean().reset_index()\n",
        "\n",
        "print(\"First few rows of aggregated customer data:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're preparing our data for customer segmentation:\n",
        "\n",
        "1. We selected three key features for segmentation: 'customer_lifetime_value', 'revenue', and 'quantity'. These features give us a good representation of a customer's value and purchasing behavior.\n",
        "\n",
        "2. We then grouped the data by 'customer_id' and calculated the mean of these features for each customer. This gives us a single row per customer, summarizing their overall behavior.\n",
        "\n",
        "3. The result, stored in the variable `X`, is a new DataFrame where each row represents a unique customer, and the columns are the average values of our selected features for that customer.\n",
        "\n",
        "This aggregation step is crucial because it allows us to perform customer-level analysis rather than transaction-level analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X[features])\n",
        "\n",
        "print(\"First few rows of scaled data:\")\n",
        "print(X_scaled[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're scaling our features using StandardScaler:\n",
        "\n",
        "1. **Scaling** is a crucial preprocessing step for many machine learning algorithms, including K-means clustering. It ensures that all features contribute equally to the result.\n",
        "\n",
        "2. **StandardScaler** transforms the data so that it has a mean of 0 and a standard deviation of 1. This is also known as **standardization** or **z-score normalization**.\n",
        "\n",
        "3. We apply the scaler to our features and store the result in `X_scaled`. Each column in `X_scaled` now has a mean of 0 and standard deviation of 1.\n",
        "\n",
        "4. The print statement shows us the first few rows of our scaled data. You'll notice that the values are now centered around 0 and typically range between -3 and 3 (as most data falls within 3 standard deviations in a normal distribution).\n",
        "\n",
        "Scaling is particularly important for K-means clustering because the algorithm uses distances between points. If one feature has a much larger scale than others, it would dominate the distance calculations and skew our results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Implementing K-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine optimal number of clusters using elbow method\n",
        "inertias = []\n",
        "for k in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 11), inertias, marker='o')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xlabel('Number of clusters, k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code block implements the **elbow method** to help us determine the optimal number of clusters for our K-means algorithm:\n",
        "\n",
        "1. We run K-means clustering for different numbers of clusters (k) from 1 to 10.\n",
        "\n",
        "2. For each k, we calculate the **inertia**, which is the sum of squared distances of samples to their closest cluster center. As k increases, inertia will decrease (each point will be closer to its cluster center).\n",
        "\n",
        "3. We plot k against inertia. The resulting plot typically looks like an arm (hence \"elbow method\"). The \"elbow\" of this plot, where the rate of decrease sharply shifts, can suggest a good number of clusters to use.\n",
        "\n",
        "4. The 'random_state=42' argument ensures reproducibility of our results.\n",
        "\n",
        "Looking at this plot, we want to choose a k where adding another cluster doesn't give much better modeling of the data. This is usually at the 'elbow' point where the inertia starts decreasing more slowly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform K-means clustering\n",
        "optimal_k = 3  # Choose based on the elbow method plot\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "X['Segment'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "print(\"First few rows with segment labels:\")\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're applying the K-means clustering algorithm to our data:\n",
        "\n",
        "1. We set `optimal_k = 3` based on our interpretation of the elbow plot. This means we're dividing our customers into 3 segments.\n",
        "\n",
        "2. We create a new KMeans object with `n_clusters=optimal_k`.\n",
        "\n",
        "3. We use the `fit_predict` method to both fit the model to our data and predict the cluster for each customer. The results are added as a new 'Segment' column to our DataFrame `X`.\n",
        "\n",
        "4. The print statement shows us the first few rows of our data, now including the segment labels (0, 1, or 2) for each customer.\n",
        "\n",
        "This step completes our customer segmentation. Each customer is now assigned to one of three segments based on their lifetime value, revenue, and quantity of purchases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Analyzing Segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize segments\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(X['customer_lifetime_value'], X['revenue'], c=X['Segment'], cmap='viridis')\n",
        "plt.title('Customer Segments')\n",
        "plt.xlabel('Customer Lifetime Value')\n",
        "plt.ylabel('Revenue')\n",
        "plt.colorbar(scatter)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This visualization helps us understand how our customer segments are distributed:\n",
        "\n",
        "1. We're creating a scatter plot where each point represents a customer.\n",
        "\n",
        "2. The x-axis shows the customer lifetime value, and the y-axis shows the revenue.\n",
        "\n",
        "3. Each point is colored according to its segment (0, 1, or 2).\n",
        "\n",
        "4. The 'viridis' colormap is used to differentiate between segments.\n",
        "\n",
        "This plot allows us to visually inspect how well our segmentation has worked. We should be able to see distinct clusters of customers, which could represent different types of customers (e.g., high-value customers, average customers, low-value customers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Analyze segment characteristics\n",
        "segment_stats = X.groupby('Segment')[features].mean()\n",
        "print(\"Segment Characteristics:\")\n",
        "print(segment_stats)\n",
        "\n",
        "# Scale the features using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "segment_stats_scaled = pd.DataFrame(scaler.fit_transform(segment_stats), \n",
        "                                    columns=segment_stats.columns, \n",
        "                                    index=segment_stats.index)\n",
        "\n",
        "# Visualize scaled segment characteristics\n",
        "plt.figure(figsize=(12, 6))\n",
        "segment_stats_scaled.plot(kind='bar')\n",
        "plt.title('Segment Characteristics (Scaled to 0-1 Range)', fontsize=16)\n",
        "plt.ylabel('Scaled Average Value', fontsize=12)\n",
        "plt.xlabel('Segment', fontsize=12)\n",
        "plt.legend(title='Features', fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This final step in our segmentation analysis helps us understand the characteristics of each segment:\n",
        "\n",
        "1. We calculate the mean values of our features for each segment. This gives us a summary of what each segment looks like in terms of customer lifetime value, revenue, and quantity.\n",
        "\n",
        "2. The printed table shows these average values for each segment. This can help us label our segments (e.g., \"high-value customers\", \"frequent buyers\", etc.).\n",
        "\n",
        "3. We then create a bar plot to visualize these characteristics. Each group of bars represents a segment, and the different colored bars within each group represent our different features.\n",
        "\n",
        "This visualization makes it easy to compare segments across different features. For example, we might see that one segment has high lifetime value and revenue but lower quantity, suggesting these could be customers who make infrequent but large purchases.\n",
        "\n",
        "Understanding these segment characteristics is crucial for developing targeted marketing strategies and improving customer relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cohort Analysis\n",
        "\n",
        "In this section, we'll perform a **cohort analysis** to understand customer retention over time. Cohort analysis is a type of behavioral analytics that takes the data from a given dataset and rather than looking at all users as one unit, it breaks them into related groups for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Creating Cohorts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cohorts based on the first purchase month\n",
        "df['CohortDate'] = df.groupby('customer_id')['order_date'].transform('min').dt.to_period('M')\n",
        "df['CohortIndex'] = (df['order_date'].dt.to_period('M') - df['CohortDate']).apply(lambda x: x.n)\n",
        "\n",
        "print(\"Sample of cohort data:\")\n",
        "print(df[['customer_id', 'order_date', 'CohortDate', 'CohortIndex']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're preparing our data for cohort analysis:\n",
        "\n",
        "1. We create a 'CohortDate' column, which represents the month of a customer's first purchase. We do this by:\n",
        "   - Grouping the data by 'customer_id'\n",
        "   - Finding the minimum 'order_date' for each customer (their first purchase)\n",
        "   - Converting this date to a period representing the month\n",
        "\n",
        "2. We then create a 'CohortIndex' column, which represents how many months have passed since a customer's first purchase. We calculate this by:\n",
        "   - Subtracting the 'CohortDate' from each 'order_date'\n",
        "   - Converting the result to a number of months\n",
        "\n",
        "3. The print statement shows us a sample of our data with these new columns.\n",
        "\n",
        "This preparation allows us to group customers who made their first purchase in the same month (cohort) and track their behavior over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Calculating Retention Rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate retention rates\n",
        "cohort_data = df.groupby(['CohortDate', 'CohortIndex'])['customer_id'].nunique().reset_index()\n",
        "cohort_data = cohort_data.pivot(index='CohortDate', columns='CohortIndex', values='customer_id')\n",
        "retention_rates = cohort_data.divide(cohort_data.iloc[:, 0], axis=0)\n",
        "\n",
        "# Visualize retention rates\n",
        "plt.figure(figsize=(20,10 ))\n",
        "sns.heatmap(retention_rates, annot=True, cmap='YlGnBu', fmt='.2%')\n",
        "plt.title('Customer Retention Rates by Cohort')\n",
        "plt.xlabel('Cohort Index (Months Since First Purchase)')\n",
        "plt.ylabel('Cohort Date')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this final step of our cohort analysis, we're calculating and visualizing customer retention rates:\n",
        "\n",
        "1. We first group our data by 'CohortDate' and 'CohortIndex', counting the unique customers in each group.\n",
        "\n",
        "2. We then pivot this data to create a matrix where each row represents a cohort (based on first purchase month) and each column represents the number of months since first purchase.\n",
        "\n",
        "3. To get retention rates, we divide each value by the initial number of customers in that cohort (the first column). This gives us the percentage of customers from each cohort that made a purchase in each subsequent month.\n",
        "\n",
        "4. We visualize this data using a heatmap:\n",
        "   - Each row represents a cohort (customers who made their first purchase in a specific month)\n",
        "   - Each column represents the number of months since the first purchase\n",
        "   - The color intensity represents the retention rate (darker colors indicate higher retention)\n",
        "   - The actual retention rate percentages are annotated in each cell\n",
        "\n",
        "This visualization allows us to easily see patterns in customer retention. For example:\n",
        "- How retention rates change over time for each cohort\n",
        "- Whether newer or older cohorts have better retention\n",
        "- If there are any seasonal patterns in retention\n",
        "\n",
        "Understanding these patterns can help in developing strategies to improve customer retention and lifetime value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Churn Prediction Model\n",
        "\n",
        "In this section, we'll build a model to predict customer **churn**. Churn refers to when a customer stops doing business with a company. Predicting churn can help businesses take proactive steps to retain customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Defining Churn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a churn period (e.g., 3 months without a purchase)\n",
        "churn_threshold = pd.Timedelta(days=90)\n",
        "df['LastPurchaseDate'] = df.groupby('customer_id')['order_date'].transform('max')\n",
        "df['DaysSinceLastPurchase'] = (df['order_date'].max() - df['LastPurchaseDate']).dt.days\n",
        "df['Churned'] = (df['DaysSinceLastPurchase'] > churn_threshold.days).astype(int)\n",
        "\n",
        "print(df[['customer_id', 'LastPurchaseDate', 'DaysSinceLastPurchase', 'Churned']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're defining and calculating churn for our customers:\n",
        "\n",
        "1. We define a **churn threshold** of 90 days (3 months). This means we consider a customer to have churned if they haven't made a purchase in the last 90 days.\n",
        "\n",
        "2. We calculate the 'LastPurchaseDate' for each customer by finding the maximum 'order_date' for each customer_id.\n",
        "\n",
        "3. We then calculate 'DaysSinceLastPurchase' by subtracting the 'LastPurchaseDate' from the most recent date in our dataset.\n",
        "\n",
        "4. Finally, we create a 'Churned' column. This is 1 (True) if 'DaysSinceLastPurchase' is greater than our churn threshold, and 0 (False) otherwise.\n",
        "\n",
        "5. We print a sample of our data to see these new columns.\n",
        "\n",
        "This definition of churn will be the basis for our predictive model. It's important to note that the definition of churn can vary depending on the business and industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Preparing Features for Churn Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate data at customer level\n",
        "customer_features = df.groupby('customer_id').agg({\n",
        "    'revenue': 'mean',\n",
        "    'quantity': 'mean',\n",
        "    'discount': 'mean',\n",
        "    'customer_lifetime_value': 'first',\n",
        "    'Churned': 'first'\n",
        "})\n",
        "\n",
        "# Prepare features and target\n",
        "X = customer_features.drop('Churned', axis=1)\n",
        "y = customer_features['Churned']\n",
        "\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're preparing our data for the churn prediction model:\n",
        "\n",
        "1. We aggregate our data at the customer level. For each customer, we calculate:\n",
        "   - Mean revenue\n",
        "   - Mean quantity of items purchased\n",
        "   - Mean discount received\n",
        "   - Customer lifetime value (we take the first value as this should be constant for each customer)\n",
        "   - Churn status (again, we take the first value as this should be the same for all rows of a customer)\n",
        "\n",
        "2. We then separate our features (X) from our target variable (y):\n",
        "   - X contains all the columns except 'Churned'\n",
        "   - y contains only the 'Churned' column\n",
        "\n",
        "3. We print the first few rows of X to verify our feature set.\n",
        "\n",
        "This preparation gives us a dataset where each row represents a unique customer, with features that summarize their purchasing behavior, and a target variable indicating whether they have churned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Training and Evaluating Churn Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest model\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this final step of our churn prediction model, we're training the model and evaluating its performance:\n",
        "\n",
        "1. We split our data into training and testing sets. We use 80% of the data for training and reserve 20% for testing. This split allows us to assess how well our model generalizes to unseen data.\n",
        "\n",
        "2. We create a **Random Forest Classifier**. Random Forest is an ensemble learning method that constructs multiple decision trees and merges them to get a more accurate and stable prediction.\n",
        "\n",
        "3. We train the model on our training data using the `fit` method.\n",
        "\n",
        "4. We use the trained model to make predictions on our test set.\n",
        "\n",
        "5. Finally, we print a classification report, which includes several metrics:\n",
        "   - **Precision**: The ratio of correctly predicted positive observations to the total predicted positive observations.\n",
        "   - **Recall**: The ratio of correctly predicted positive observations to all actual positive observations.\n",
        "   - **F1-score**: The harmonic mean of Precision and Recall, providing a single score that balances both metrics.\n",
        "   - **Support**: The number of occurrences of each class in the test set.\n",
        "\n",
        "This report helps us understand how well our model is performing in predicting customer churn. We can use these insights to refine our model or to start developing strategies to prevent churn for high-risk customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Personalized Marketing Recommendation Engine\n",
        "\n",
        "In this section, we'll implement a simple recommendation engine using collaborative filtering. This can be used to suggest products to customers based on the purchasing behavior of similar customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Preparing Data for Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a user-item matrix\n",
        "user_item_matrix = df.pivot_table(index='customer_id', columns='product_id', values='quantity', fill_value=0)\n",
        "print(user_item_matrix.shape)\n",
        "print(user_item_matrix.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're preparing our data for the recommendation engine:\n",
        "\n",
        "1. We create a **user-item matrix** using the `pivot_table` function. This matrix has:\n",
        "   - Rows representing customers (customer_id)\n",
        "   - Columns representing products (product_id)\n",
        "   - Values representing the quantity of each product purchased by each customer\n",
        "\n",
        "2. We fill any missing values with 0, assuming that if a customer hasn't purchased a product, their quantity is 0.\n",
        "\n",
        "3. We print the shape of the matrix to see how many customers and products we have.\n",
        "\n",
        "4. We also print the first few rows of the matrix to see what it looks like.\n",
        "\n",
        "This user-item matrix is the foundation for our collaborative filtering approach. It allows us to see patterns in purchasing behavior across all customers and products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Implementing Collaborative Filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate item-item similarity matrix\n",
        "item_similarity = cosine_similarity(user_item_matrix.T)\n",
        "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
        "\n",
        "print(item_similarity_df.shape)\n",
        "print(item_similarity_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this step, we're implementing a key part of our collaborative filtering system:\n",
        "\n",
        "1. We calculate the **item-item similarity matrix** using cosine similarity. This measures how similar each product is to every other product based on customer purchasing patterns.\n",
        "\n",
        "2. We use `cosine_similarity` on the transpose of our user-item matrix. This is because we want to compare columns (products) rather than rows (customers).\n",
        "\n",
        "3. We convert the similarity matrix to a DataFrame for easier handling, with product IDs as both row and column labels.\n",
        "\n",
        "4. We print the shape of the similarity matrix (which should be number of products by number of products).\n",
        "\n",
        "5. We also print the first few rows to see what the similarity scores look like.\n",
        "\n",
        "This similarity matrix allows us to find products that are often purchased together or by the same customers. We'll use this to generate our recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Generating Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_recommendations(customer_id, n=5):\n",
        "    # Get the user's purchase history\n",
        "    user_purchases = user_item_matrix.loc[customer_id]\n",
        "    \n",
        "    # Calculate the weighted sum of similarities\n",
        "    weighted_sum = item_similarity_df.mul(user_purchases, axis=0).sum(axis=1)\n",
        "    \n",
        "    # Remove items the user has already purchased\n",
        "    recommendations = weighted_sum[user_purchases == 0].sort_values(ascending=False)\n",
        "    \n",
        "    return recommendations.head(n)\n",
        "\n",
        "# Example: Get recommendations for a specific customer\n",
        "customer_id = df['customer_id'].iloc[0]  # Just picking the first customer as an example\n",
        "recommendations = get_recommendations(customer_id)\n",
        "print(f\"Top 5 product recommendations for customer {customer_id}:\")\n",
        "print(recommendations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this final step, we're creating a function to generate personalized product recommendations:\n",
        "\n",
        "1. We define a function `get_recommendations` that takes a customer ID and the number of recommendations to generate.\n",
        "\n",
        "2. Inside the function:\n",
        "   - We get the customer's purchase history from our user-item matrix.\n",
        "   - We calculate a weighted sum of similarities. This weighs the similarity of each product by how much the user has purchased of similar products.\n",
        "   - We remove products the user has already purchased, as we don't need to recommend these.\n",
        "   - We sort the remaining products by their weighted similarity score and return the top n.\n",
        "\n",
        "3. We then demonstrate the function by generating recommendations for the first customer in our dataset.\n",
        "\n",
        "This recommendation system suggests products based on what similar customers have purchased. It's a simple but effective way to generate personalized recommendations that can increase cross-selling and customer satisfaction.\n",
        "\n",
        "The output shows the top 5 recommended product IDs for the chosen customer, along with their recommendation scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've covered several advanced e-commerce analytics techniques:\n",
        "\n",
        "1. **Customer Segmentation**: We used K-means clustering to group customers based on their purchasing behavior.\n",
        "2. **Cohort Analysis**: We analyzed customer retention rates over time for different cohorts.\n",
        "3. **Churn Prediction**: We built a Random Forest model to predict which customers are likely to churn.\n",
        "4. **Recommendation Engine**: We implemented a simple collaborative filtering system to generate personalized product recommendations. This can enhance the customer experience and increase cross-selling opportunities.\n",
        "\n",
        "These techniques provide valuable insights into customer behavior and can guide decision-making in areas such as marketing, product development, and customer retention strategies.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Data-Driven Decision Making**: Each of these analyses provides actionable insights that can directly inform business strategies. For example, customer segments can guide targeted marketing campaigns, while churn predictions can trigger personalized retention efforts.\n",
        "\n",
        "2. **Customer-Centric Approach**: By focusing on customer behavior patterns, purchase history, and preferences, these techniques help businesses align their offerings more closely with customer needs and expectations.\n",
        "\n",
        "3. **Predictive Power**: The churn prediction model demonstrates how historical data can be used to forecast future behavior, allowing for proactive rather than reactive business strategies.\n",
        "\n",
        "4. **Personalization at Scale**: The recommendation engine shows how businesses can provide personalized experiences to a large customer base, potentially increasing customer satisfaction and sales.\n",
        "\n",
        "### Practical Applications\n",
        "\n",
        "1. **Marketing**: Use customer segments to create targeted advertising campaigns. Tailor email marketing content based on a customer's segment and recommended products.\n",
        "\n",
        "2. **Customer Service**: Prioritize high-value customers or those at risk of churning for premium support services.\n",
        "\n",
        "3. **Product Development**: Use insights from customer segments and popular product combinations (from the recommendation engine) to inform new product development or bundling strategies.\n",
        "\n",
        "4. **Retention Strategies**: Implement targeted retention campaigns for customers predicted to churn, offering personalized incentives based on their purchase history and segment.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further enhance this analysis, consider the following steps:\n",
        "\n",
        "1. **Feature Engineering**: Create more complex features that might better capture customer behavior, such as frequency of purchases, average time between purchases, or seasonal purchasing patterns.\n",
        "\n",
        "2. **Advanced Modeling**: Experiment with more sophisticated machine learning models, such as gradient boosting machines for churn prediction or neural networks for product recommendations.\n",
        "\n",
        "3. **A/B Testing**: Implement A/B tests to measure the effectiveness of personalized recommendations or targeted retention strategies derived from these analyses.\n",
        "\n",
        "4. **Real-Time Analytics**: Develop systems to update these analyses in real-time as new customer data comes in, allowing for more dynamic and responsive business strategies.\n",
        "\n",
        "5. **Incorporate External Data**: Integrate external data sources, such as economic indicators or competitor pricing, to provide more context to customer behavior patterns.\n",
        "\n",
        "6. **Explainable AI**: For the more complex models like Random Forest, implement techniques to interpret the model's decisions, helping to build trust and derive more actionable insights.\n",
        "\n",
        "By leveraging these advanced analytics techniques and continuously refining the approach based on new data and business feedback, e-commerce businesses can gain a significant competitive advantage in understanding and serving their customers.\n",
        "\n",
        "Remember, the field of e-commerce analytics is constantly evolving. Stay updated with the latest techniques and tools to maintain a competitive edge in the market."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
